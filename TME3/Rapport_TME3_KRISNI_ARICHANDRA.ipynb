{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TME 3 - Descente de gradient**\n",
    "\n",
    "Membres du binôme :\n",
    "- KRISNI Almehdi (3800519)\n",
    "- ARICHANDRA Santhos (3802651)\n",
    "\n",
    "L'objectif de ce TME est d'expérimenter sur la descente de gradient dans le cadre de la régression linéaire et de la régression logistique. L'espace d'entrée est de dimension *d*, les labels sont des réels (linéaire) ou dans {-1, 1} (logistique). L'espace de recherche fonctionnel est considéré par **w**, un vecteur réel de dimension *d* et avec *n* le nombre d'exemples. La notion de biais ne sera pas retenu dans le cadre de ce TME, donc pas de poids à **w0**.\n",
    "\n",
    "On s'occupe dans un premier temps à importer les fichiers nécessaires à la création du rapport et implémenter les fonctions de coût dans le fichier *tme3*.\n",
    "\n",
    "### **Implémentation des fonctions de coût**\n",
    "\n",
    "Les méthodes allant être codées ne contiennent aucune boucle, ce qui permet de meilleurs résultats en terme d'exécution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    }
   ],
   "source": [
    "# Import du fichier tme3.py\n",
    "from tme3 import *\n",
    "\n",
    "# Rechargement automatique des fichiers importés (vu dans l'UE de Data Science en L3)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le fichier *mltools.py* fourni dans le cadre du TME, on dispose de plusieurs méthodes nous facilitant la tâche, comme par exemple la méthode *gen_arti* permettant la création rapide d'un dataset.\n",
    "\n",
    "On code dans un premier temps les méthodes **mse(w,x,y)** et **reglog(w,x,y)**, retournant respectivement l'erreur sur les moindres carrés, soit pour des données linéaires, et l'erreur de la regression logistique. Les formules utilisées dans la rédaction sont celles vues en cours. \n",
    "\n",
    "On code ensuite les gradients de chacune des erreurs, dont les formes matricielles sont obtenues en dérivant les fonctions de coût par chaque poids du vecteur w.\n",
    "\n",
    "La méthode **check_fonctions** permet de vérifier si les méthodes codées précedemment sont correctes en réalisant une série d'assert sur les valeurs obtenues. Si un des asserts se retrouve être faux, alors la méthode se termine et renvoie une erreur. On vérifie donc si le code mis en place est correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification des fonctions\n",
    "check_fonctions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il semblerait que les méthodes codées soient toutes correctes. On a essayé de modifier un des résultats retournés, comme par exemple l'ajout d'une multiplication par un réel dans la MSE, ce qui a provoqué une AssertError.\n",
    "\n",
    "Afin de tester l'exactitude de nos fonctions de gradient, on écrit la fonction **grad_check(f,f_grad,N=100,eps=1E-2)** allant tirer N points au hasard et vérifie le calcul du gradient sur les N points en moyenne. On doit rajouter un paramètre *epsilon*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La différence moyenne entre le gradient MSE et celui du DL de Taylor est : 28.77\n",
      "\n",
      "On affiche les différentes valeurs :\n",
      "Gradient MSE\t\tGradient Taylor\n",
      "117610.02\t\t117512.01\n",
      "1432.42\t\t1431.21\n",
      "78701.22\t\t78635.61\n",
      "37582.72\t\t37551.36\n",
      "106020.72\t\t105932.36\n",
      "14654.50\t\t14642.25\n",
      "27598.08\t\t27575.04\n",
      "53823.78\t\t53778.89\n",
      "52227.12\t\t52183.56\n",
      "184.32\t\t184.16\n",
      "21119.28\t\t21101.64\n",
      "49105.92\t\t49064.96\n",
      "0.00\t\t0.00\n",
      "3439.78\t\t3436.89\n",
      "26458.18\t\t26436.09\n",
      "17280.88\t\t17266.44\n",
      "38938.98\t\t38906.49\n",
      "108290.50\t\t108200.25\n",
      "12244.48\t\t12234.24\n",
      "63908.58\t\t63855.29\n",
      "4768.00\t\t4764.00\n",
      "50654.50\t\t50612.25\n",
      "21119.28\t\t21101.64\n",
      "43152.00\t\t43116.00\n",
      "5773.68\t\t5768.84\n",
      "16381.38\t\t16367.69\n",
      "1182.00\t\t1181.00\n",
      "41723.62\t\t41688.81\n",
      "2327.92\t\t2325.96\n",
      "8073.52\t\t8066.76\n",
      "2674.50\t\t2672.25\n",
      "88727.92\t\t88653.96\n",
      "10050.82\t\t10042.41\n",
      "1182.00\t\t1181.00\n",
      "47581.38\t\t47541.69\n",
      "24250.50\t\t24230.25\n",
      "1432.42\t\t1431.21\n",
      "84645.12\t\t84574.56\n",
      "103774.98\t\t103688.49\n",
      "71112.58\t\t71053.29\n",
      "420.72\t\t420.36\n",
      "108290.50\t\t108200.25\n",
      "19152.00\t\t19136.00\n",
      "117610.02\t\t117512.01\n",
      "33658.18\t\t33630.09\n",
      "2327.92\t\t2325.96\n",
      "44.08\t\t44.04\n",
      "9367.68\t\t9359.84\n",
      "44.08\t\t44.04\n",
      "3858.48\t\t3855.24\n",
      "106020.72\t\t105932.36\n",
      "74858.82\t\t74796.41\n",
      "27598.08\t\t27575.04\n",
      "3439.78\t\t3436.89\n",
      "12244.48\t\t12234.24\n",
      "8708.58\t\t8701.29\n",
      "44604.42\t\t44567.21\n",
      "58758.00\t\t58709.00\n",
      "420.72\t\t420.36\n",
      "2674.50\t\t2672.25\n",
      "6875.52\t\t6869.76\n",
      "3045.12\t\t3042.56\n",
      "25342.32\t\t25321.16\n",
      "58758.00\t\t58709.00\n",
      "53823.78\t\t53778.89\n",
      "22138.98\t\t22120.49\n",
      "36250.50\t\t36220.25\n",
      "110584.32\t\t110492.16\n",
      "72973.68\t\t72912.84\n",
      "18204.42\t\t18189.21\n",
      "33658.18\t\t33630.09\n",
      "38938.98\t\t38906.49\n",
      "90805.38\t\t90729.69\n",
      "44.08\t\t44.04\n",
      "80658.48\t\t80591.24\n",
      "8708.58\t\t8701.29\n",
      "420.72\t\t420.36\n",
      "290.50\t\t290.25\n",
      "18204.42\t\t18189.21\n",
      "6875.52\t\t6869.76\n",
      "15505.92\t\t15492.96\n",
      "84645.12\t\t84574.56\n",
      "34942.32\t\t34913.16\n",
      "110584.32\t\t110492.16\n",
      "290.50\t\t290.25\n",
      "4301.22\t\t4297.61\n",
      "9367.68\t\t9359.84\n",
      "67462.50\t\t67406.25\n",
      "19152.00\t\t19136.00\n",
      "78701.22\t\t78635.61\n",
      "13023.78\t\t13012.89\n",
      "10758.00\t\t10749.00\n",
      "86674.50\t\t86602.25\n",
      "0.00\t\t0.00\n",
      "3858.48\t\t3855.24\n",
      "2674.50\t\t2672.25\n",
      "37582.72\t\t37551.36\n",
      "46080.88\t\t46042.44\n",
      "37582.72\t\t37551.36\n",
      "4301.22\t\t4297.61\n"
     ]
    }
   ],
   "source": [
    "# Utilisation de grad_check avec la MSE\n",
    "mG, logG = grad_check(mse, mse_grad)\n",
    "\n",
    "print(\"La différence moyenne entre le gradient MSE et celui du DL de Taylor est :\", \"{:.2f}\".format(mG[0][0]))\n",
    "print(\"\\nOn affiche les différentes valeurs :\\nGradient MSE\\t\\tGradient Taylor\")\n",
    "for i in range(len(logG)) :\n",
    "    print(\"{:.2f}\".format(logG[i][0]), \"{:.2f}\".format(logG[i][1]), sep=\"\\t\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque donc que les valeurs des gradients sont pratiquement égales à chaque niveau puisque sur 100 exemples, la différence moyenne est de 31,91. Les exemples dont les dimensions sont importantes sont la principale source de différence tandis que pour les faibles dimensions, on retrouve des valeurs égales au dixième près.\n",
    "\n",
    "On réalise une nouvelle fois un test avec cette fois le gradient de la régression logistique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La différence moyenne entre le gradient de la Reg Log et celui du DL de Taylor est : nan\n",
      "\n",
      "On affiche les différentes valeurs des gradients:\n",
      "Reg Log\t\tTaylor\n",
      "17.00\t\t17.00\n",
      "94.00\t\tnan\n",
      "56.00\t\t56.00\n",
      "33.00\t\t33.00\n",
      "8.00\t\t8.00\n",
      "50.00\t\t50.00\n",
      "30.00\t\t30.00\n",
      "76.00\t\t76.00\n",
      "22.00\t\t22.00\n",
      "82.00\t\tnan\n",
      "8.00\t\t8.00\n",
      "21.00\t\t21.00\n",
      "13.00\t\t13.00\n",
      "99.00\t\tnan\n",
      "5.00\t\t5.00\n",
      "32.00\t\t32.00\n",
      "26.00\t\t26.00\n",
      "42.00\t\t42.00\n",
      "46.00\t\t46.00\n",
      "49.00\t\t49.00\n",
      "82.00\t\tnan\n",
      "30.00\t\t30.00\n",
      "89.00\t\tnan\n",
      "53.00\t\t53.00\n",
      "24.00\t\t24.00\n",
      "0.00\t\t0.00\n",
      "35.00\t\t35.00\n",
      "76.00\t\t76.00\n",
      "64.00\t\t64.00\n",
      "72.00\t\t72.00\n",
      "28.00\t\t28.00\n",
      "92.00\t\tnan\n",
      "93.00\t\tnan\n",
      "32.00\t\t32.00\n",
      "93.00\t\tnan\n",
      "7.00\t\t7.00\n",
      "78.00\t\t78.00\n",
      "16.00\t\t16.00\n",
      "22.00\t\t22.00\n",
      "85.00\t\tnan\n",
      "85.00\t\tnan\n",
      "7.00\t\t7.00\n",
      "88.00\t\tnan\n",
      "42.00\t\t42.00\n",
      "60.00\t\t60.00\n",
      "56.00\t\t56.00\n",
      "15.00\t\t15.00\n",
      "76.00\t\t76.00\n",
      "38.00\t\t38.00\n",
      "70.00\t\t70.00\n",
      "12.00\t\t12.00\n",
      "2.00\t\t2.00\n",
      "59.00\t\t59.00\n",
      "40.00\t\t40.00\n",
      "38.00\t\t38.00\n",
      "70.00\t\t70.00\n",
      "24.00\t\t24.00\n",
      "91.00\t\tnan\n",
      "58.00\t\t58.00\n",
      "4.00\t\t4.00\n",
      "83.00\t\tnan\n",
      "92.00\t\tnan\n",
      "46.00\t\t46.00\n",
      "37.00\t\t37.00\n",
      "77.00\t\t77.00\n",
      "66.00\t\t66.00\n",
      "4.00\t\t4.00\n",
      "77.00\t\t77.00\n",
      "98.00\t\tnan\n",
      "73.00\t\t73.00\n",
      "67.00\t\t67.00\n",
      "28.00\t\t28.00\n",
      "3.00\t\t3.00\n",
      "33.00\t\t33.00\n",
      "87.00\t\tnan\n",
      "20.00\t\t20.00\n",
      "2.00\t\t2.00\n",
      "18.00\t\t18.00\n",
      "47.00\t\t47.00\n",
      "55.00\t\t55.00\n",
      "88.00\t\tnan\n",
      "20.00\t\t20.00\n",
      "39.00\t\t39.00\n",
      "6.00\t\t6.00\n",
      "40.00\t\t40.00\n",
      "73.00\t\t73.00\n",
      "88.00\t\tnan\n",
      "17.00\t\t17.00\n",
      "44.00\t\t44.00\n",
      "52.00\t\t52.00\n",
      "49.00\t\t49.00\n",
      "76.00\t\t76.00\n",
      "85.00\t\tnan\n",
      "64.00\t\t64.00\n",
      "14.00\t\t14.00\n",
      "33.00\t\t33.00\n",
      "19.00\t\t19.00\n",
      "52.00\t\t52.00\n",
      "36.00\t\t36.00\n",
      "2.00\t\t2.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Almehdi\\Documents\\UPMC\\M1_DAC\\S2\\ML\\TME\\TME3\\tme3.py:48: RuntimeWarning: overflow encountered in exp\n",
      "  return np.log(1 + np.exp((-y) * np.dot(x,w)))\n",
      "d:\\Almehdi\\Documents\\UPMC\\M1_DAC\\S2\\ML\\TME\\TME3\\tme3.py:94: RuntimeWarning: invalid value encountered in subtract\n",
      "  gradT = ((f(w + eps, x[i], y)) - (f(w,x[i],y))) / eps\n"
     ]
    }
   ],
   "source": [
    "# Utilisation de grad_check avec la Reg Log\n",
    "mG, logG = grad_check(reglog, reglog_grad)\n",
    "\n",
    "print(\"La différence moyenne entre le gradient de la Reg Log et celui du DL de Taylor est :\", \"{:.2f}\".format(mG[0][0]))\n",
    "print(\"\\nOn affiche les différentes valeurs des gradients:\\nReg Log\\t\\tTaylor\")\n",
    "for i in range(len(logG)) :\n",
    "    print(\"{:.2f}\".format(logG[i][0]), \"{:.2f}\".format(logG[i][1]), sep=\"\\t\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats sont remarquables puisque la différence moyenne sur 100 exemples est considérée comme nulle au centième près. On en conclut donc que pour la MSE ou la régression logistique, les gradients calculés sont corrects. <br/>Passons maintenant au sujet de la descente de gradient.\n",
    "\n",
    "### **Descente de gradient**\n",
    "\n",
    "On implémente avant toute chose la fonction **descente_gradient(datax,datay,f_loss,f_grad,eps,maxIter)** réalisant une descente de gradient afin d'optimiser le coût de *f_loss*, dont le gradient est donné par *f_grad* sur les données *datax*, les labels *datay*, un pas de descente *eps* et un nombre *iter* d'itérations.\n",
    "<br/>La fonction retournera le paramètre optimal **w** trouvé, la liste des **w** calculés et les valeurs de la fonction de coût au fur et à mesure des itérations.\n",
    "\n",
    "Nous avons vu en cours plusieurs formes de descente de gradient :\n",
    "- batch\n",
    "- stochastique\n",
    "- mini-batch\n",
    "On code donc une fonction par forme de descente de gradient.\n",
    "\n",
    "On réalise un premier test avec le gradient MSE sur la descente de gradient en batch. On génère avant un dataset allant être utilisé pour toutes les expériences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du dataset\n",
    "datax, datay = gen_arti(epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descente de gradient en batch - MSE\n",
    "w, logW, logF = descente_gradient_batch(datax, datay, mse, mse_grad, eps=0.1, maxIter=100)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0882f64853dad4c2959728fd5765a8f4f31953d4de2b55ec484562a906c43e55"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

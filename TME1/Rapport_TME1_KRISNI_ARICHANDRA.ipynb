{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rapport TME1 - Arbres de décision, sélection de modèles\n",
    "\n",
    "Membres du binôme :\n",
    "- KRISNI Almehdi\n",
    "- ARICHANDRA Santhos\n",
    "\n",
    "Dans ce TME, on aborde les aspects essentiels des arbres de décision. Les arbres sont des modèles de classification hiérarchique, en d'autres termes, pour des exemples de la forme **x** = *(x1, x2, ..., xd)*, on associe à chaque noeud de l'arbre un test sur une des dimensions de *xi* de la forme *xi* <= s avec s une valeur réelle. Le test indique le noeud fils devant être sélectionné. La feuille atteinte après l'ensemble des tests est la classe prédite pour un exemple.\n",
    "\n",
    "### **L'essentiel sur les arbres de décision**\n",
    "\n",
    "On cherche à implémenter les fonctions utiles au calcul du partitionnement optimal, soit les fonctions de calcul d'entropie. Toutes les méthodes sont situées dans le fichier *entropie.py* du répertoire TME1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import du fichier entropie.py\n",
    "from methodesTME import *\n",
    "\n",
    "# Import utilitaires\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On code la méthode *entropie* permettant à partir d'un vecteur (une liste ou un vecteur numpy), de calculer l'entropie de ce vecteur. Une entropie importante représente un ensemble de données hétérogène tandis qu'une entropie nulle représente un ensemble homogéne, soit composé d'une seule et unique valeur.\n",
    "\n",
    "On réalise quelques tests avec la méthode crée sur des vecteurs différents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vecteur : [8, 6, 4, 0, 3, 2, 3, 3, 4, 6] \tEntropie : 1.6957425341696346\n",
      "Vecteur : [8, 10, 6, 1, 0, 10, 10, 5, 9, 7] \tEntropie : 1.9730014063936123\n",
      "Vecteur : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] \tEntropie : 0.0\n"
     ]
    }
   ],
   "source": [
    "# Tests d'entropie\n",
    "vT1 = [random.randint(0, 10) for _ in range(10)]\n",
    "vT2 = [random.randint(0, 10) for _ in range(10)]\n",
    "vT3 = [1 for _ in range(10)]\n",
    "\n",
    "print(\"Vecteur :\", vT1, \"\\tEntropie :\", entropie(vT1))\n",
    "print(\"Vecteur :\", vT2, \"\\tEntropie :\", entropie(vT2))\n",
    "print(\"Vecteur :\", vT3, \"\\tEntropie :\", entropie(vT3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On code la méthode *entropie_cond* qui à partir d'une liste de vecteurs, calcule l'homogéinité de la partition obtenue. Il s'agit de la moyenne pondérée des entropies des sous-ensembles obtenus.\n",
    "\n",
    "On réalise un test avec les vecteurs créés à l'étape précédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble de vecteurs :\n",
      "[8, 6, 4, 0, 3, 2, 3, 3, 4, 6]\n",
      "[8, 10, 6, 1, 0, 10, 10, 5, 9, 7]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "Entropie conditionnelle de la partition : 1.2229146468544156\n"
     ]
    }
   ],
   "source": [
    "# Test d'entropie conditionnelle\n",
    "print(\"Ensemble de vecteurs :\", vT1, vT2, vT3, sep=\"\\n\")\n",
    "print(\"\\nEntropie conditionnelle de la partition :\", entropie_cond([vT1, vT2, vT3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de charger un extrait des données de la base imdb, on dispose d'un code fourni par le sujet du TME permettant le chargement d'une zone précise du fichier. On exécute le code afin de conserver en mémoire les données et ne pas avoir à les recharger à chaque utilisation.\n",
    "\n",
    "On réalise aussi différents affichages afin de mieux comprendre les données utilisées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les différents champs des données dans data sont :\n",
      "{0: 'Sci-Fi', 1: 'Crime', 2: 'Romance', 3: 'Animation', 4: 'Music', 5: 'Comedy', 6: 'War', 7: 'Horror', 8: 'Film-Noir', 9: 'Adult', 10: 'News', 11: 'Reality-TV', 12: 'Thriller', 13: 'Western', 14: 'Mystery', 15: 'Short', 16: 'Talk-Show', 17: 'Drama', 18: 'Action', 19: 'Documentary', 20: 'Musical', 21: 'History', 22: 'Family', 23: 'Adventure', 24: 'Fantasy', 25: 'Game-Show', 26: 'Sport', 27: 'Biography', 28: 'Duree', 29: 'couleur', 30: 'annee', 31: 'budget', 32: 'nb_votes', 33: 'rating'}\n",
      "\n",
      "Le nombre d'exemples dans la base de données est de : 4587\n",
      "\n",
      "La attributs du premier exemple sont :\n",
      "['0.00', '0.00', '0.00', '0.00', '0.00', '1.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '1.00', '0.00', '0.00', '0.00', '0.00', '1.00', '0.00', '0.00', '0.00', '0.00', '108.00', '2.00', '1988.00', '12324000.00']\n",
      "\n",
      "Le label du premier exemple est : -1\n",
      "\n",
      "Le titre du premier film de data est : 'Crocodile' Dundee II\n"
     ]
    }
   ],
   "source": [
    "# Data : tableau (films, features), id2titles : dictionnaire id -> titre,\n",
    "# fields : id feature -> nom\n",
    "[data, id2titles, fields]= pickle.load(open(\"imdb_extrait.pkl\", \"rb\"))\n",
    "# La derniere colonne est le vote\n",
    "datax = data[:,:32]\n",
    "datay = np.array([1 if x [33] > 6.5 else -1 for x in data])\n",
    "\n",
    "# On affiche le nom des différents champs\n",
    "print(\"Les différents champs des données dans data sont :\", fields, sep=\"\\n\")\n",
    "\n",
    "# On affiche la taille, la première instance de la base de données récupérée et son label\n",
    "print(\"\\nLe nombre d'exemples dans la base de données est de :\", len(datax))\n",
    "print(\"\\nLa attributs du premier exemple sont :\", [\"{0:0.2f}\".format(i) for i in datax[0]], sep='\\n')\n",
    "print(\"\\nLe label du premier exemple est :\", datay[0])\n",
    "\n",
    "# On affiche le premier titre\n",
    "print(\"\\nLe titre du premier film de data est :\", id2titles[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient donc une base de données conservée dans la variable data. Chaque ligne de data représente un film précis et on dispose de plusieurs attributs à propos de ce dernier comme son genre, sa durée, son année de sortie ou son budget par exemple.\n",
    "\n",
    "Le genre du film est géré par un one-hot encoding sur les 28 premiers attributs. Si pour un de ses attributs on retrouve la valeur 1, alors cela signifie que le film appartient au genre représenté par l'attribut. On peut, comme pour le premier exemple, trouver un film appartenant à plusieurs catégories.\n",
    "\n",
    "Le vecteur datay contient le score de chacun des films et ce score est égal à 1 si la moyenne de ses notes est supérieur à 6.5, ou égal à -1 dans le cas contraire.\n",
    "\n",
    "On s'intéresse maintenant à l'entropie du vecteur datay contenant les labels de chacun des films, puis à l'entropie conditionnelle des vecteurs des 28 premiers attributs de chacun des exemples de la base de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'entropie de Y est : 0.6840731540145955 \n",
      "\n",
      "Les entropies conditionnelles en fonction de chaque attribut (0 à 27) sont :\n",
      "[0.67994028 0.68360929 0.6840712  0.68252302 0.68406157 0.67298199\n",
      " 0.67776557 0.66834313 0.68159391 0.68407315 0.68407315 0.68407315\n",
      " 0.67613533 0.68266372 0.68398548 0.6837022  0.68407315 0.64202544\n",
      " 0.67267917 0.68320271 0.68236801 0.67462006 0.68276896 0.68388467\n",
      " 0.68249102 0.68407315 0.68407204 0.66945832]\n"
     ]
    }
   ],
   "source": [
    "# On affiche l'entropie de datay\n",
    "entY = entropie(datay)\n",
    "print(\"L'entropie de Y est :\", entY, '\\n')\n",
    "\n",
    "# Entropie conditionnelle pour chaque attribut binaire (attributs 0 à 27)\n",
    "ent_cond = np.zeros(28)\n",
    "\n",
    "for i in range(28) :\n",
    "    # On cherche quels sont les exemples dont l'attribut i est égal à 0 ou 1\n",
    "    ind0 = np.where(np.array([x[i] for x in datax]) == 0)\n",
    "    ind1 = np.where(np.array([x[i] for x in datax]) == 1)\n",
    "\n",
    "    # On récupère les différents labels\n",
    "    lab0 = [datay[j] for j in ind0][0]\n",
    "    lab1 = [datay[j] for j in ind1][0]\n",
    "\n",
    "    # On sauvegarde l'entropie conditionnelle sur l'attribut i\n",
    "    ent_cond[i] = entropie_cond([lab0, lab1])\n",
    "\n",
    "# On affiche les différentes entropies conditionnelles calculées\n",
    "print(\"Les entropies conditionnelles en fonction de chaque attribut (0 à 27) sont :\", ent_cond, sep='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On comprend alors que la différence entre l'entropie de Y et l'entropie de Y conditionnellement à un des attributs i est le gain d'information obtenu grâce à l'utilisation de l'attribut i. On cherche donc à maximiser ce gain d'information en réalisant une comparaison entre l'entropie de Y et les entropies conditionnelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le gain d'information pour chacun des attributs i est :\n",
      "[0.00413287340530788, 0.00046386323157165954, 1.9551364983660946e-06, 0.0015501296269213594, 1.1586434889521158e-05, 0.011091159441398268, 0.006307582810820889, 0.015730023292110373, 0.002479248608378226, 0.0, 0.0, 0.0, 0.007937823466661387, 0.0014094297134645828, 8.767079884108853e-05, 0.0003709584397669774, 0.0, 0.04204771856211831, 0.011393982540818137, 0.0008704410364122062, 0.0017051473508644666, 0.009453090064388325, 0.001304193427266287, 0.00018848299466323049, 0.0015821370532405066, 0.0, 1.1176414111968569e-06, 0.014614838257637763]\n",
      "\n",
      "Le meilleur attribut pour réaliser la séparation est Drama ( en position 17 ).\n"
     ]
    }
   ],
   "source": [
    "# On crée la liste contenant la différence d'entropie entre Y et Y conditionnellement à l'attribut i\n",
    "difEnt = (entY - ent_cond).tolist()\n",
    "indAtt= difEnt.index(max(difEnt))\n",
    "\n",
    "# On affiche les résultats obtenus\n",
    "print(\"Le gain d'information pour chacun des attributs i est :\", difEnt, sep='\\n')\n",
    "print(\"\\nLe meilleur attribut pour réaliser la séparation est\", fields[indAtt], \"( en position\", indAtt, \").\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Quelques expériences préliminaires**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sur et sous apprentissage**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Validation croisée et sélection de modèle**"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0882f64853dad4c2959728fd5765a8f4f31953d4de2b55ec484562a906c43e55"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
